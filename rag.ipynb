{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load the tokenizer and model\n",
    "model_0 = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer_0 = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "model_1 = AutoModelForCausalLM.from_pretrained(\"Hiran03/GPT2finetuned\")\n",
    "tokenizer_1 = AutoTokenizer.from_pretrained(\"Hiran03/GPT2finetuned\")\n",
    "    \n",
    "\n",
    "# # Input query\n",
    "# query = \"what is microstructure\"\n",
    "\n",
    "# # Tokenize the input\n",
    "\n",
    "\n",
    "# print(\"Pretrained :\")\n",
    "\n",
    "# inputs = tokenizer_0(query, return_tensors=\"pt\")\n",
    "# # Generate response\n",
    "# output_tokens = model_0.generate(\n",
    "#     inputs[\"input_ids\"],\n",
    "#     attention_mask=inputs[\"attention_mask\"],\n",
    "#     pad_token_id=model_0.config.eos_token_id,\n",
    "#     max_length = 100,\n",
    "# )\n",
    "# response = tokenizer_0.decode(output_tokens[0], skip_special_tokens=True)\n",
    "# print(response)\n",
    "\n",
    "# print (\"\\n \\n\")\n",
    "# print(\"Fine-tuned : \")\n",
    "\n",
    "# inputs = tokenizer_1(query, return_tensors=\"pt\")\n",
    "# # Generate response\n",
    "# output_tokens = model_1.generate(\n",
    "#     inputs[\"input_ids\"],\n",
    "#     attention_mask=inputs[\"attention_mask\"],\n",
    "#     pad_token_id=model_1.config.eos_token_id,\n",
    "#     max_length = 100,\n",
    "# )\n",
    "# response = tokenizer_1.decode(output_tokens[0], skip_special_tokens=True)\n",
    "# print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8ff1e9b11564e1eb69fdd8f501da77d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/298k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b9e06c9a71546f1b9d239ba391d3991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hiran\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hiran\\.cache\\huggingface\\hub\\models--Hiran03--GPT2finetuned. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Hiran03/GPT2finetuned/commit/d87db242afba1331bc82b9597cb7a8fec6d3e0ec', commit_message='Upload tokenizer', commit_description='', oid='d87db242afba1331bc82b9597cb7a8fec6d3e0ec', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Hiran03/GPT2finetuned', endpoint='https://huggingface.co', repo_type='model', repo_id='Hiran03/GPT2finetuned'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Push to Hugging Face Hub\n",
    "model_name = \"GPT2finetuned\"\n",
    "# model_1.push_to_hub(model_name)\n",
    "# tokenizer_1.push_to_hub(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: paper_1.pdf\n",
      "Processing file: paper_2.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# Path to the folder containing PDF files\n",
    "pdf_folder = r\"papers\"\n",
    "\n",
    "text = \"\"\n",
    "# Iterate over all files in the folder\n",
    "for filename in os.listdir(pdf_folder):\n",
    "    if filename.endswith(\".pdf\"):  # Process only PDF files\n",
    "        pdf_path = os.path.join(pdf_folder, filename)\n",
    "        print(f\"Processing file: {filename}\")\n",
    "        \n",
    "        # Read the PDF\n",
    "        reader = PdfReader(pdf_path)\n",
    "        \n",
    "        # Extract text from all pages\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "        \n",
    "\n",
    "with open(os.path.join(os.getcwd(),\"rag_text.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(text)\n",
    "import text_preprocess as text_preprocess\n",
    "text_preprocess.preprocessing(os.path.join(os.getcwd(),\"rag_text.txt\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model successfully initialized!\n",
      "FAISS index saved at: faiss_index\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.vectorstores.utils import DistanceStrategy\n",
    "from sentence_transformers import SentenceTransformer, models\n",
    "\n",
    "# Step 1: Load the Text File\n",
    "file_path = \"rag_text.txt\"  # Replace with your text file path\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Step 2: Split the Text into Chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \",\", \" \"],\n",
    "    chunk_size=900,  # Maximum chunk size\n",
    "    chunk_overlap=200  # Overlap between chunks\n",
    ")\n",
    "chunks = text_splitter.split_text(text)\n",
    "\n",
    "# Convert each chunk into a LangChain Document\n",
    "docs_processed = [Document(page_content=chunk) for chunk in chunks]\n",
    "\n",
    "# Step 3: Set Up Sentence Transformer\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# Initialize HuggingFaceEmbeddings\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs={\"device\": \"cpu\"},  # Change to 'cuda' if GPU is available\n",
    "    encode_kwargs={\"normalize_embeddings\": True},  # Normalize embeddings for cosine similarity\n",
    ")\n",
    "\n",
    "print(\"Embedding model successfully initialized!\")\n",
    "# Step 4: Create FAISS Vector Store\n",
    "KNOWLEDGE_VECTOR_DATABASE = FAISS.from_documents(\n",
    "    docs_processed,\n",
    "    embedding_model,\n",
    "    distance_strategy=DistanceStrategy.COSINE\n",
    ")\n",
    "\n",
    "# Step 5: Save the FAISS Index\n",
    "faiss_file_path = \"faiss_index\"\n",
    "KNOWLEDGE_VECTOR_DATABASE.save_local(faiss_file_path)\n",
    "print(f\"FAISS index saved at: {faiss_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time in a distant land, he might have looked like he knew what he was getting into! However, here, he had a face which was as cold as ice and dark as the sky beneath his eyes.\n",
      "\n",
      "But what he saw was so frightening as to leave a very frightened glance on the girl's face! Even with his knowledge of the magic sword, the demon was a very small man! What kind of magic might she be and would she give him a magic that could\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "# Step 1: Load the GPT-2 model and tokenizer\n",
    "model_name = \"Hiran03/GPT2finetuned\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Step 2: Create a Hugging Face pipeline\n",
    "hf_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_length=100)\n",
    "\n",
    "# Step 3: Wrap the pipeline in LangChain's HuggingFacePipeline\n",
    "llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
    "\n",
    "# Step 4: Generate text\n",
    "prompt = \"Once upon a time in a distant land,\"\n",
    "output = llm(prompt)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "# Step 1: Load the Text File\n",
    "file_path = \"rag_text.txt\"  # Replace with your text file path\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Step 2: Split the Text into Chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \",\", \" \"],\n",
    "    chunk_size=1000,  # Maximum chunk size\n",
    "    chunk_overlap=200  # Overlap between chunks\n",
    ")\n",
    "chunks = text_splitter.split_text(text)\n",
    "\n",
    "# Step 3: Set Up Sentence Transformer\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# Initialize HuggingFaceEmbeddings\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs={\"device\": \"cpu\"},  # Change to 'cuda' if GPU is available\n",
    "    encode_kwargs={\"normalize_embeddings\": True},  # Normalize embeddings for cosine similarity\n",
    ")\n",
    "\n",
    "query = \"what is crystallinity\"\n",
    "# Step 1: Load the GPT-2 model and tokenizer\n",
    "model_name = \"Hiran03/GPT2finetuned\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Step 2: Create a Hugging Face pipeline\n",
    "hf_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_length=1024, truncation = True)\n",
    "\n",
    "# Step 3: Wrap the pipeline in LangChain's HuggingFacePipeline\n",
    "llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
    "vectorstore = FAISS.from_texts(chunks,embedding_model)\n",
    "chain = RetrievalQA.from_llm(llm=llm, retriever=vectorstore.as_retriever())\n",
    "result = chain(query)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'what is crystallinity',\n",
       " 'result': \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nContext:\\naqueous solution of nh46mo7o244h2o precursor6 however these\\nmethods have limitations in growing largesize single crystals with\\nrapid growth rates now as they report in nature materials he jiang\\nand lleagues have effectively utilized the wellknown cz method\\nfor the growth of 2d tmd particularly mos2 ref 7 they achieved\\nthe production of single crystals exceeding 15 cm in size with an\\nimpressive growth rate of 75 m s1 marking a clear advancement\\nover previous techniques\\nthe key aspect of the 2d cz method is its use of a liquidliquid\\ninterface spreading mechanism to suppress nucleation and enhance\\ncrystal growth rate this enables the growth of centimetrescale\\nsingle crystals with low defect density moreover the weak adhesion\\nbetween the mos2 crystal and the substrate facilitates the trans \\nfer process making it advantageous for device fabrication the\\nexperimental process involves four main steps predeposition\\netching spreading and inplane crystallization fig 1b first for\\n\\nContext:\\nthe bhj nanomorphology nsisting of the donor polymer d18cl and\\nthe acceptor n3 owing to hydrogen bonding interactions between the\\nhydrophilic glylate side chains of atb2o and n3 the authors can\\nprecisely regulate the mponents thinfilm crystallization in fact\\nthe ternary d184clatb2on3 blends form a bhj gradient vertical\\nphase separation nsisting of a donorrich d184cl phase close to\\nthe substrate a uniform bhj region in the middle and an acceptorrich\\nphase n3 with enhanced crystallinity near the top of the active layer\\nfig 1 this nfiguration mbines the advantages of both bhj\\nand layerbylayer deposition where the former maximizes exciton splitting while the latter allows the formation of pure donor and\\nacceptor phases to enhance charge transport when introduced in the\\nternary blend atb2o promotes higher crystallinity and a longerrange\\norder of n3 with long diffusion lengths approaching 50 nm facilitat\\ning exciton separation and higher electron mobilities in the vertical\\n\\nContext:\\nnology this includes green solvent formulation scalable donor and\\nacceptor materials and printable and flexible electrodes aiming for\\nstable devices under realworld operation\\nfrances furlan nila gasparini\\ndepartment of chemistry and centre for processable electronics\\nimperial llege london london uk\\nemail ngaspariniimperialacuknature materialsdoiorg101038s4156302402100x\\nnews views twodimensional materials\\ntwodimensional czochralski growth\\nseoungki lee jonghyun ahn\\nthe twodimensional czochralski growth\\nmethod enables the rapid production of\\nlargearea singlecrystal mos2 effectively\\nalleviating the issues related to defect\\ndensity and scalability for devices based on\\ntwodimensional materials\\nthe czochralski cz method disvered by jan czochralski in 1915 has\\nevolved into a fundamental technique in single crystal growth fig 1a1\\nits significance was solidified when gordon teal at bell laboratory\\nsuccessfully applied it to grow single crystals of germanium making a\\n\\nContext:\\nfer process making it advantageous for device fabrication the\\nexperimental process involves four main steps predeposition\\netching spreading and inplane crystallization fig 1b first for\\npredeposition of mos2 moo3 powder on the glass surface melts and\\nreacts with na2o in the glass substrate forming numerous na2mo2o7\\neutectic droplets this prevents rapid sublimation of moo3 ensuring\\nan adequate mo source for subsequent steps initial liquid crystal \\nlization is induced by a sulfur atmosphere leading to multilayer\\nmos2 domains floating on molten glass send introducing o2\\ninto the tube results in a solidtoliquid process of predeposited\\nmos2 involving etching and a eutectic reaction third molten glass\\ndecreases the ntact angle of eutectic droplets spreading a 2d\\nliquid and forming its film fourth after the spreading process an\\ncheck for updates\\n1 predeposition 2 etching pr ocess\\n4 inplane cr ystallization 3 liquid 2d spr eading 2d c z growth 3d c z growth\\n\\nQuestion: what is crystallinity\\nHelpful Answer: the density can increase exponentially\\n2 nd, even as the moo3 powder is soluble\\nin the same layer 1 h2o8o24\\n\\n1 H 4 O 3\\n\\n4 H 5 CaCO 3\\n\\n4 O 6\\n\\n2 S f m u t\"}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
